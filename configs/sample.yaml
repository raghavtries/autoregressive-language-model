model:
  vocab_size: 50257
  d_model: 384
  n_layers: 6
  n_heads: 6
  d_ff: 1536
  dropout: 0.1
  block_size: 256

train:
  batch_size: 32
  micro_batch_size: 8
  max_steps: 1000
  learning_rate: 3e-4
  warmup_steps: 100
  grad_clip: 1.0
  amp: true
  grad_accumulation_steps: 4

data:
  train_path: "data/train.txt"
  val_path: "data/val.txt"
  block_size: 256

tokenizer:
  vocab_size: 50257
  special_tokens:
    pad: "<pad>"
    bos: "<bos>"
    eos: "<eos>"
    unk: "<unk>" 