model:
  vocab_size: 100
  d_model: 64
  n_layers: 2
  n_heads: 4
  d_ff: 128
  dropout: 0.1
  block_size: 16

train:
  learning_rate: 3e-4
  warmup_steps: 10
  max_steps: 50
  batch_size: 4
  grad_accumulation_steps: 1
  grad_clip: 1.0
  amp: false
  use_compile: false  # Set to true to test torch.compile
  use_gradient_checkpointing: false  # Set to true to test gradient checkpointing
  log_interval: 5
  eval_interval: 20
  save_interval: 25

data:
  train_path: data/train.txt
  val_path: data/val.txt
  block_size: 16

tokenizer:
  vocab_size: 100 