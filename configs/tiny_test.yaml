# Tiny configuration for testing training loop
seed: 42

model:
  vocab_size: 100
  d_model: 64
  n_layers: 2
  n_heads: 4
  d_ff: 256
  dropout: 0.1
  block_size: 16

train:
  batch_size: 4
  micro_batch_size: 2
  max_steps: 100
  learning_rate: 1e-3
  warmup_steps: 10
  grad_clip: 1.0
  amp: false
  grad_accumulation_steps: 2

data:
  train_path: "data/train.txt"
  val_path: "data/val.txt"
  block_size: 16

tokenizer:
  vocab_size: 100
  special_tokens:
    pad: "<pad>"
    bos: "<bos>"
    eos: "<eos>"
    unk: "<unk>"

# Training settings
log_interval: 5
eval_interval: 20
save_interval: 50
log_dir: "logs"
num_workers: 0
pin_memory: false 